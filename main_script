#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Jun 24 11:56:10 2023

@author: nyk, GPT-4
"""

from nltk.tokenize import sent_tokenize
from datetime import datetime
from pymongo import MongoClient, errors
from sentence_transformers import SentenceTransformer
import numpy as np
import os
import logging
import time
from nltk.corpus import stopwords
import shutil
import subprocess


# Configure logging
logging.basicConfig(filename='knowledge_base.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class DBHandler:
    def __init__(self):
        self.client = None
        self.db = None
        self.collection = None

    def get_mongodb_connection(self, uri, retry_count=5):
        for i in range(retry_count):
            try:
                client = MongoClient(uri)
                return client
            except errors.ConnectionFailure as e:
                if i < retry_count - 1:  # i is zero indexed
                    print(f"Failed to connect to MongoDB (attempt {i + 1}), retrying in 5 seconds.")
                    time.sleep(5)
                    continue
                else:
                    print("Failed to connect to MongoDB, please check your MongoDB connection.")
                    logging.error(f"Failed to connect to MongoDB: {e}")
                    raise SystemExit()

    def connect(self):
        if self.client is None:
            self.client = self.get_mongodb_connection('mongodb://localhost:#####/')
            self.db = self.client['knowledge_base3']
            self.collection = self.db['proto1']

    def disconnect(self):
        if self.client is not None:
            self.client.close()
            self.client = None
            self.db = None
            self.collection = None

db_handler = DBHandler()

# Initialize the model to create sentence embeddings
model = SentenceTransformer('/Users/nyk/sentence-sim/all-MiniLM-L6-v2')

# Takes a text string as an input and replaces all newline characters (\n) with a space, effectively making the text a single line.
def preprocess_text(text):
    return text.replace('\n', ' ')

# This function calculates the cosine similarity between two vectors a and b by taking their dot product and dividing it by the product of their norms (lengths). 
# This is a measure of how similar the two vectors are.
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


def get_most_relevant_learnings(user_prompt, top_n=5, threshold=0.4):
    db_handler.connect()
    user_prompt_embedding = model.encode([user_prompt], show_progress_bar=False)[0]
    learning_similarity = []
    for entry in db_handler.collection.find():
        if entry.get('is_question', False):
            continue
        content_embedding = np.array(entry['content_embedding'])
        similarity = cosine_similarity(user_prompt_embedding, content_embedding)
        if similarity > threshold:
            learning_similarity.append((entry['content'], similarity))
    
    # Sort by similarity and get the top_n
    learning_similarity.sort(key=lambda x: x[1], reverse=True)
    most_relevant_learnings = [learning for learning, similarity in learning_similarity[:top_n]]
    
    # If there's no learning found, return a helpful message
    if not most_relevant_learnings:
        db_handler.disconnect()
        return "Not enough context found. Please try rewording your phrase or asking a different question."
    
    db_handler.disconnect()
    return most_relevant_learnings


def compute_and_store_embedding(text):
    # Compute the embedding for the text
    db_handler.connect()
    embedding = model.encode([text])[0].tolist()

    # Check if this text is already in the database
    if db_handler.collection.count_documents({'content': text}, limit = 1) == 0:
        # If it's not, add it to the database
        db_handler.collection.insert_one({'content': text, 'content_embedding': embedding, 'is_question': False})
        db_handler.disconnect()
        return True  # Return True when a new sentence is added
    else:
        print(f"Entry '{text}' already exists in the database.")
    db_handler.disconnect()
    return False  # Return False when the sentence already exists in the database


# Constants...
stop_words = set(stopwords.words('english'))

def compute_and_store_embedding_and_backup_learnings():
    # Update the database with new learnings
    with open('/Users/nyk/Desktop/proto/learnings.txt', 'r') as file:
        learnings = file.read()

    sentences = sent_tokenize(learnings)
    new_sentences = []  # List to hold new sentences that were added to the database

    for sentence in sentences:
        sentence = preprocess_text(sentence)
        if compute_and_store_embedding(sentence):
            new_sentences.append(sentence)

    # Get the directory of 'learnings.txt'
    learnings_dir = os.path.dirname('/Users/nyk/Desktop/proto/learnings.txt')

    # Check if learnings.txt is empty - if it is do not create a backup
    if os.stat("/Users/nyk/Desktop/proto/learnings.txt").st_size != 0:
        # Create a backup before clearing the file
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_filename = f'learnings_backup_{timestamp}.txt'

        # Form the path of the backup file
        backup_path = os.path.join(learnings_dir, 'backups', backup_filename)

        shutil.copy('/Users/nyk/Desktop/proto/learnings.txt', backup_path)

        # Clear the file after creating a backup
        open('/Users/nyk/Desktop/proto/learnings.txt', 'w').close()
    else:
        print("Learnings file is empty. No backup created.")
    return new_sentences


def reveal_command(verbose):
    db_handler.connect()
    entries = []
    for doc in db_handler.collection.find({'is_question': False}):
        if verbose:
            entries.append(str(doc))
        else:
            entries.append(doc['content'])
    db_handler.disconnect()
    return "\n".join(entries)

def eliminate_command(content_to_delete):
    db_handler.connect()
    result = db_handler.collection.delete_one({'content': content_to_delete})
    db_handler.disconnect()
    if result.deleted_count > 0:
        return f"Entry '{content_to_delete}' removed from the database."
    else:
        return f"Entry '{content_to_delete}' not found in the database."

def ask_command(question):
    most_relevant_learning = get_most_relevant_learnings(question)
    return f"Most relevant learning: {most_relevant_learning}"

def update_command(original_content, new_content):
    db_handler.connect()
    # Try to find the original entry
    original_entry = db_handler.collection.find_one({'content': original_content})
    if original_entry:
        # If the original entry is found, update its content and its embedding
        new_embedding = model.encode([new_content], show_progress_bar=False)[0].tolist()
        db_handler.collection.update_one({'_id': original_entry['_id']}, {'$set': {'content': new_content, 'content_embedding': new_embedding}})
        db_handler.disconnect()
        return f"Entry '{original_content}' updated to '{new_content}'."
    else:
        db_handler.disconnect()
        return f"Entry '{original_content}' not found."

def search_command(query):
    db_handler.connect()
    results = db_handler.collection.find({'content': {'$regex': query}})
    matches = [result['content'] for result in results]
    db_handler.disconnect()
    return "\n".join(matches)

def start_mongodb():
    mongo_process = subprocess.Popen(['mongod', '--dbpath', 'mongodb-data/', '--port', '27018'])
    # You can customize the command based on your MongoDB installation and configuration
    return mongo_process

if __name__ == '__main__':
    # Start MongoDB process
    mongodb_process = start_mongodb()

    # Terminate MongoDB process when the script exits
    mongodb_process.terminate()
